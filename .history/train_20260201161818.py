import torch
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# --- 1. é…ç½®æ¨¡å‹å’Œåˆ†è¯å™¨ ---
model_name = "Qwen/Qwen2-7B-Instruct"  # æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©Qwen2ï¼Œå¦‚ "Qwen/Qwen2-7B-Instruct"
dataset_path = "./processed_data"  # ä¸Šä¸€æ­¥å¤„ç†å¥½çš„æ•°æ®é›†è·¯å¾„
output_dir = "./qwen-7b-text2sql-adapter" # å¾®è°ƒåé€‚é…å™¨æƒé‡ä¿å­˜ç›®å½•

# --- 2. QLoRA é…ç½® ---
# ä½¿ç”¨4-bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=False,
)

# --- 3. LoRA é…ç½® ---
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=32,
    bias="none",
    task_type="CAUSAL_LM",
    # Qwen1.5 çš„ç›®æ ‡æ¨¡å—ï¼ŒQwen2 å¯èƒ½ç¨æœ‰ä¸åŒï¼Œä½†é€šå¸¸æ˜¯è¿™äº›
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] 
)

# --- 4. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# è§£å†³Qwençš„pad_tokené—®é¢˜
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" 

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto", # è‡ªåŠ¨å°†æ¨¡å‹åˆ†é…åˆ°å¯ç”¨æ˜¾å¡
    trust_remote_code=True
)

# å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
model = prepare_model_for_kbit_training(model)
# åº”ç”¨LoRAé…ç½®
model = get_peft_model(model, peft_config)

# --- 5. åŠ è½½æ•°æ®é›† ---
print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
dataset = load_from_disk(dataset_path)

# --- 6. é…ç½®è®­ç»ƒå‚æ•° ---
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,                     # è®­ç»ƒè½®æ¬¡
    per_device_train_batch_size=2,          # æ¯ä¸ªGPUçš„æ‰¹å¤„ç†å¤§å°
    gradient_accumulation_steps=4,          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæœ‰æ•ˆæ‰¹å¤§å° = 2 * 4 = 8
    optim="paged_adamw_32bit",              # ä½¿ç”¨åˆ†é¡µä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
    save_steps=50,                          # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    logging_steps=10,                       # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    learning_rate=2e-4,                     # å­¦ä¹ ç‡
    weight_decay=0.001,
    fp16=False,                             # å¦‚æœæ‚¨çš„GPUæ”¯æŒï¼Œè¯·ä½¿ç”¨bf16=True
    bf16=True,
    max_grad_norm=0.3,
    max_steps=-1,                           # å¦‚æœè®¾ç½®ä¸ºæ­£æ•°ï¼Œåˆ™è¦†ç›–num_train_epochs
    warmup_ratio=0.03,                      # é¢„çƒ­æ¯”ä¾‹
    group_by_length=True,                   # æŒ‰é•¿åº¦åˆ†ç»„æ ·æœ¬ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
    lr_scheduler_type="cosine",             # å­¦ä¹ ç‡è°ƒåº¦å™¨
    report_to="tensorboard",
    evaluation_strategy="steps",            # æ¯Næ­¥è¿›è¡Œä¸€æ¬¡éªŒè¯
    eval_steps=50                           # æ¯50æ­¥éªŒè¯ä¸€æ¬¡
)

# --- 7. åˆå§‹åŒ–å¹¶å¼€å§‹è®­ç»ƒ ---
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=1024,                     # æœ€å¤§åºåˆ—é•¿åº¦
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,                          # æ˜¯å¦å°†å¤šä¸ªçŸ­æ ·æœ¬æ‰“åŒ…æˆä¸€ä¸ªé•¿æ ·æœ¬
)

print("ğŸš€ å¼€å§‹å¾®è°ƒï¼")
trainer.train()

# --- 8. ä¿å­˜æœ€ç»ˆçš„é€‚é…å™¨ ---
print("âœ… å¾®è°ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆçš„é€‚é…å™¨...")
trainer.save_model(output_dir)